# JobVerse Data Crawler

C√¥ng c·ª• crawl d·ªØ li·ªáu vi·ªác l√†m IT th·ª±c t·ª´ c√°c trang tuy·ªÉn d·ª•ng Vi·ªát Nam.

## T√≠nh nƒÉng

‚úÖ **Crawl t·ª´ nhi·ªÅu ngu·ªìn:**
- TopCV.vn - Trang tuy·ªÉn d·ª•ng h√†ng ƒë·∫ßu VN
- ITviec.com - Chuy√™n IT jobs
- C√≥ th·ªÉ m·ªü r·ªông th√™m ngu·ªìn kh√°c

‚úÖ **D·ªØ li·ªáu thu th·∫≠p:**
- Th√¥ng tin c√¥ng vi·ªác (title, description, requirements, benefits)
- Th√¥ng tin c√¥ng ty (name, logo, description, website, size, industry)
- M·ª©c l∆∞∆°ng (salary range)
- ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác
- Skills y√™u c·∫ßu
- Categories
- Company logos (download v·ªÅ local)

‚úÖ **T·ª± ƒë·ªông:**
- Normalize location (H·ªì Ch√≠ Minh, H√† N·ªôi, ƒê√† N·∫µng, Remote)
- Extract category t·ª´ job title
- Extract experience level (JUNIOR, MIDDLE, SENIOR)
- Extract skills t·ª´ title
- Parse salary range
- Generate requirements v√† benefits
- Download company logos

## C√†i ƒë·∫∑t

### 1. C√†i ƒë·∫∑t Python dependencies:

```bash
pip install -r requirements.txt
```

### 2. C·∫•u h√¨nh database (cho seeder):

T·∫°o file `.env` ho·∫∑c export environment variables:

```bash
export DB_HOST=localhost
export DB_PORT=5432
export DB_NAME=jobverse
export DB_USER=postgres
export DB_PASSWORD=your_password
```

Ho·∫∑c s·ª≠a tr·ª±c ti·∫øp trong file `seed_database.py`:

```python
DB_CONFIG = {
    'host': 'localhost',
    'port': '5432',
    'database': 'jobverse',
    'user': 'postgres',
    'password': 'your_password'
}
```

## S·ª≠ d·ª•ng

### B∆∞·ªõc 1: Crawl d·ªØ li·ªáu

```bash
python job_crawler.py
```

**Output:**
- File `crawled_jobs.json` - Ch·ª©a to√†n b·ªô d·ªØ li·ªáu crawled
- Folder `crawled_images/companies/` - Ch·ª©a logo c√¥ng ty

**Tu·ª≥ ch·ªânh s·ªë l∆∞·ª£ng:**

M·ªü file `job_crawler.py` v√† s·ª≠a d√≤ng cu·ªëi:

```python
crawler.run(max_pages_per_site=20)  # TƒÉng s·ªë n√†y ƒë·ªÉ crawl nhi·ªÅu h∆°n
```

- 10 pages = ~200-300 jobs
- 20 pages = ~400-600 jobs
- 50 pages = ~1000-1500 jobs

### B∆∞·ªõc 2: Import v√†o database

```bash
python seed_database.py
```

Script s·∫Ω t·ª± ƒë·ªông:
1. K·∫øt n·ªëi database
2. Insert categories
3. Insert skills
4. Insert companies (v·ªõi logo)
5. Insert jobs + job_skills

**L∆∞u √Ω:** Script t·ª± ƒë·ªông ki·ªÉm tra duplicate, ch·ªâ insert d·ªØ li·ªáu m·ªõi.

## Crawl li√™n t·ª•c

ƒê·ªÉ crawl li√™n t·ª•c v√† c√≥ c√†ng nhi·ªÅu data c√†ng t·ªët:

### C√°ch 1: Ch·∫°y l·∫∑p l·∫°i th·ªß c√¥ng

```bash
# L·∫ßn 1
python job_crawler.py
python seed_database.py

# L·∫ßn 2 (sau v√†i gi·ªù/ng√†y)
python job_crawler.py
python seed_database.py
```

### C√°ch 2: Shell script t·ª± ƒë·ªông

T·∫°o file `crawl_loop.sh`:

```bash
#!/bin/bash
while true; do
    echo "üöÄ Starting crawl cycle..."
    python job_crawler.py
    python seed_database.py
    echo "‚úÖ Cycle complete. Waiting 6 hours..."
    sleep 21600  # 6 hours
done
```

Ch·∫°y:
```bash
chmod +x crawl_loop.sh
./crawl_loop.sh
```

### C√°ch 3: Windows batch script

T·∫°o file `crawl_loop.bat`:

```batch
@echo off
:loop
echo Starting crawl cycle...
python job_crawler.py
python seed_database.py
echo Waiting 6 hours...
timeout /t 21600 /nobreak
goto loop
```

Ch·∫°y:
```batch
crawl_loop.bat
```

### C√°ch 4: Cron job (Linux/Mac)

```bash
# Crawl m·ªói 6 gi·ªù
0 */6 * * * cd /path/to/doan1 && python job_crawler.py && python seed_database.py
```

## C·∫•u tr√∫c d·ªØ li·ªáu

### crawled_jobs.json

```json
{
  "jobs": [
    {
      "title": "Senior Backend Developer",
      "company": "FPT Software",
      "location": "H·ªì Ch√≠ Minh",
      "salary_min": 25000000,
      "salary_max": 45000000,
      "description": "...",
      "requirements": "...",
      "benefits": "...",
      "category": "Backend",
      "employment_type": "FULL_TIME",
      "experience_level": "SENIOR",
      "skills": ["Java", "Spring Boot", "MySQL"],
      "deadline": "2025-02-15",
      "source": "TopCV",
      "source_url": "https://..."
    }
  ],
  "companies": [
    {
      "name": "FPT Software",
      "logo": "/crawled_images/companies/abc123.jpg",
      "description": "...",
      "website": "https://...",
      "size": "1000+",
      "industry": "Information Technology"
    }
  ],
  "categories": ["Backend", "Frontend", "Mobile", ...],
  "skills": ["Java", "React", "Python", ...],
  "crawled_at": "2025-12-20T10:30:00",
  "total_jobs": 542,
  "total_companies": 87
}
```

## M·ªü r·ªông th√™m ngu·ªìn

ƒê·ªÉ crawl t·ª´ th√™m trang kh√°c, th√™m method v√†o class `JobCrawler`:

```python
def crawl_new_site(self, max_pages=10):
    """Crawl jobs from NewSite.com"""
    print("üîç Crawling NewSite.com...")
    base_url = "https://newsite.com"

    for page in range(1, max_pages + 1):
        url = f"{base_url}/jobs?page={page}"
        response = self.session.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Parse job items...
        # Add to self.jobs

        time.sleep(random.uniform(2, 4))
```

Sau ƒë√≥ g·ªçi trong `run()`:

```python
def run(self, max_pages_per_site=10):
    self.crawl_topcv(max_pages_per_site)
    self.crawl_itviec(max_pages_per_site)
    self.crawl_new_site(max_pages_per_site)  # Th√™m d√≤ng n√†y
```

## X·ª≠ l√Ω l·ªói

### L·ªói k·∫øt n·ªëi database:

```
‚ùå Database connection failed: ...
```

**Gi·∫£i ph√°p:**
1. Ki·ªÉm tra PostgreSQL ƒëang ch·∫°y: `pg_ctl status`
2. Ki·ªÉm tra credentials trong DB_CONFIG
3. Ki·ªÉm tra firewall/port 5432

### L·ªói crawl b·ªã block:

```
Error on page X: 403 Forbidden
```

**Gi·∫£i ph√°p:**
1. TƒÉng delay gi·ªØa requests: `time.sleep(random.uniform(5, 10))`
2. Th√™m proxy rotation
3. Th√™m random User-Agent

### L·ªói parse HTML:

```
Error parsing job item: ...
```

**Gi·∫£i ph√°p:**
- Website c√≥ th·ªÉ ƒë√£ thay ƒë·ªïi c·∫•u tr√∫c HTML
- C·∫ßn update selectors trong code

## Performance Tips

1. **TƒÉng t·ªëc crawl:**
   - Gi·∫£m delay: `time.sleep(1)` thay v√¨ `time.sleep(3)`
   - S·ª≠ d·ª•ng async/await v·ªõi `aiohttp`
   - Multi-threading

2. **Tr√°nh duplicate:**
   - Seeder t·ª± ƒë·ªông check duplicate
   - C√≥ th·ªÉ th√™m unique constraint tr√™n job slug

3. **Optimize database:**
   - Batch insert (ƒë√£ implement v·ªõi `execute_values`)
   - Create indexes tr√™n columns th∆∞·ªùng query

## Statistics

V·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh (`max_pages_per_site=20`):

- **Crawl time:** ~5-10 ph√∫t
- **Jobs crawled:** 400-600 jobs
- **Companies:** 80-150 companies
- **Categories:** 10-15 categories
- **Skills:** 30-50 skills
- **Images downloaded:** 80-150 logos
- **File size:** ~500KB-2MB JSON

## License

MIT License - Free to use for JobVerse project

## Author

Quang Thang - December 2025
